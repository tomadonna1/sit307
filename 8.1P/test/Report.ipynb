{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Week 7: SVM models'\n",
        "---"
      ],
      "id": "9e23d79e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following information is summarized from Géron (2019) book.\n",
        "\n",
        "![picture](1.png)\n",
        "\n",
        "This picture demonstrate the decision boundaries. On the left, there are 3 decision boundaries, the green one doesn't amount to anything, the purple and red one, although they separate the data quite nicely on the training set, but won't generalize well on test set.\n",
        "\n",
        "On the right, it is the SVM classier, the decision boundary separates 2 classes and stays away as far as possible from the closet training instances. We called this **large margin classification**, where the decision boundary is determined by the closest instances to the decision boundary which is called **support vectors**.\n",
        "\n",
        "\n",
        "## 7.1 Soft Margin Classification\n",
        "What we see in the above example is **hard margin classification** where all instance must be on the right side. However, it only works if the data is linearly separable, and sensitive to outliers. The picture below will demonstrate that if we add one instance on of the wrong class, the decision boundary will not be drawn, or drawn but not generalize well.\n",
        "\n",
        "![picture](2.png)\n",
        "\n",
        "To avoid this, we need to use a more flexible model or **soft margin classification** model, where it finds a good balance for the decision boundary margin and avoid *margin violations* (instances in the middle of the margin or wrong side).\n",
        "\n",
        "The hyperparameter `C` will control the trade-off between maximizing the margin and minimizing the classification error. The picture below shows the 2 case scenarios. On the left where `C` is small, it might not fit the data well (margin violations), but generalize the model by maximizing the margin between classes. On the right where `C` is high, the model is less tolerant to misclassified because of smaller margin, but this could lead to overfitting. \n",
        "\n",
        "## 7.2 Nonlinear SVM Classification\n",
        "SVM has something called the **kernel trick**, where it computes the dot product of the original data points `X` and the transformed `X'` in a high dimensional space without explicitly transforming the data into that space.\n",
        "\n",
        "The transformed data depends on the **kernel functions**, and there are a few popular ones like linear, polynomial, RBF and sigmoid. \n",
        "\n",
        "### 7.2.1 Polynomial Kernel\n",
        "It works similar to adding polynomial features without adding polynomial features. The picture below will show SVM classifier with different degree of polynomial. \n"
      ],
      "id": "f8806ca1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.15)\n",
        "from sklearn.svm import SVC\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "    ])\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ],
      "id": "7ca8969f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![picture](3.png)\n",
        "\n",
        "If the model is overfit, we can reduce the polynomial degree, increase it of underfit. The hyperparameter `coef0` controls how much the model is influenced by high-degree vs low-degree of polynomials.\n",
        "\n",
        "#### Tips on finding the right hyperparameter\n",
        "Use grid search multiple times, with each time narrow down the search to find the optimal hyperparameter.\n",
        "\n",
        "### 7.2.2 Gaussian RBF Kernel\n",
        "To understand about this kernel function, we need to understand about **similar features**, which measures how much each instance resembles a particular *landmark*. Say we have an 1D dataset and two landmarks at $x_1=-2$ and $x_1 = 1$. The similar function will be the **RGaussian Radial Basis Function (RBF)** with the equation is \n",
        "$$\n",
        "\\phi_{\\gamma}(\\mathbf{x}, \\ell) = \\exp\\left(-\\gamma \\|\\mathbf{x} - \\ell\\|^2\\right)\n",
        "$$\n",
        "where $\\gamma=0.3$.\n",
        "\n",
        "This is a bell-shaped function range from 0 (very far from the landmark) to 1 (at the landmark). Take instance $x=-1$, it is located 1 distance from the first landmark and 2 from the second. Therefore, the new features are $x_2=\\text{exp}(-0.3 \\times 1^2) \\approx 0.74$ and $x_3=\\text{exp}(-0.3 \\times 2^2) \\approx 0.3$.\n",
        "\n",
        "![picture](4.png)\n",
        "\n",
        "This is just an example, but we create a landmark at the location of all the instances in the dataset. This brings us back the **Gaussian RBF Kernel** where we can do all of this without creating new features. \n"
      ],
      "id": "d16f3a5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "    ])\n",
        "rbf_kernel_svm_clf.fit(X, y)"
      ],
      "id": "fcee1fec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperparameter gamma $\\gamma$ acts as an regularization that controls the bell-shaped. Increase $\\gamma$ makes the bell-shaped curve narrower coz each instance's range of influence is smaller. Therefore the decision boundary becomes more irregular and wiggling around individual instances. Conversely, a smaller $\\gamma$ makes the bell-shaped curve wider, instances have larger range of influence and the decision boundary is smoother. \n",
        "\n",
        "![picture](5.png)\n",
        "\n",
        "### 7.3 SVM Regression\n",
        "SVM Regression works inversely to SVM Classification. Instead of trying to fit the largest possible street between two classes while limiting the violations, SVM Regression *fit* as many instances as possible on the street while limiting the violations. The width of the street is controlled by hyperparameter epsilon $\\epsilon$.\n",
        "\n",
        "![picture](6.png)\n",
        "\n",
        "The same things goes for nonlinear SVM.\n",
        "\n",
        "### 7.4 Under the Hood\n",
        "### 7.4.1 Decision Function and Predictions\n",
        "In a binary classification, the decision function is $\\mathbf{w}^T\\mathbf{x} + b = w_1x_1 + ... + w_nx_n + b$. If the predicted class $\\hat{y}$ is positive then 1, negative then 0. More formally, the equation is\n",
        "$$\n",
        "\\hat{y} = \\begin{cases}\n",
        "    0 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b < 0, \\\\\n",
        "    1 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "![picture](7.png)\n",
        "\n",
        "The decision boundary is set to 0, the dashed lines are the points where the decision function is $\\pm 1$. The dashed lines are parallel to decision boundary, and formed a margin around it. Training the linear SVM classifier means to find $\\mathbf{w}$ and $b$ to make the margin as wide as possible to avoid margin violations (hard margin) or limiting them (soft margin).\n",
        "\n",
        "### 7.4.2 Training objective\n",
        "![picture](8.png)\n",
        "\n",
        "We can see that we need to minimize $||w||$ to get large margin. We define $t^{(i)} = -1$ for negative instances ($y^{(i)} =0$) and $t^{(i)} = 1$ for positive instances ($y^{(i)} =1$), then we can express the constraint as $t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\geq 1$ for all instances.\n",
        "\n",
        "The objective of hard margin linear SVM classifier is\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\underset{\\text{w,b}}{minimize}\\quad & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}  = \\frac{1}{2}||w|| \\\\\n",
        "\\text{subject to} \\quad & t^{(i)} \\left( \\mathbf{w}^T \\mathbf{x}^{(i)} + b \\right) \\geq 1 \\quad \\text{for } i = 1, 2, \\dots, m\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "To get the soft margin objective, we need to introduce a *slack variable* $\\zeta$, which measures how mich instance $i^{th}$ is allowed to violate the margin. Now we have 2 conflicting objective, make $\\zeta$ as small as possible to reduce margin violations, and make $||w||$ as small as possible to increase the margin. This is where hyperparameter `C` comes in, where it allows us to define the trade off between the two objectives.\n",
        "\n",
        "The soft margin linear SVM classifier is \n",
        "$$\n",
        "\\begin{align*}\n",
        "\\underset{\\text{w,b},\\zeta}{minimize} \\quad & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}  + C\\sum_m^{i=1}\\zeta^{(i)} \\\\\n",
        "\\text{subject to} \\quad & t^{(i)} \\left( \\mathbf{w}^T\\mathbf{x}^{(i)} + b \\right) \\geq 1 - \\zeta^{(i)} \\quad \\text{and } \\zeta^{(i)} \\geq 0 \\text{ for } i = 1, 2, \\dots, m\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Both hard and soft margin problems are convex quadratic optimizations, I don't really understand the math so ima skip it :).\n",
        "\n",
        "### 7.4.3 The Dual Problem\n",
        "We know that the primal problem in SVM is to optimize the margin while minimizing classification error.\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\underset{\\text{w,b},\\zeta}{minimize} \\quad & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}  + C\\sum_m^{i=1}\\zeta^{(i)} \\\\\n",
        "\\text{subject to} \\quad & t^{(i)} \\left( \\mathbf{w}^T\\mathbf{x}^{(i)} + b \\right) \\geq 1 - \\zeta^{(i)} \\quad \\text{and } \\zeta^{(i)} \\geq 0 \\text{ for } i = 1, 2, \\dots, m\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The dual problem derived from the primal problem where dual solution uses **Lagrange multipliers** $\\alpha_i$ to determine which training examples will become support vectors and thus define the optimal decision boundary.\n",
        "\n",
        "The dual from of the linear SVM objective is\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\underset{\\alpha}{minimize} \\quad \\frac{1}{2}\\sum^m_{i=1} \\sum^m_{j=1} \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)} - \\sum^m_{i=1} \\alpha^{(i)} \\\\\n",
        "\n",
        "\\text{subject to } \\alpha^{(i)} \\geq 0 \\text{ for all } i = 1,2,...,m \\text{ and } \\sum^m_{i=1} \\alpha^{(i)}t^{(i)} = 0 \n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Once we find the minimized vector $\\mathbf{\\alpha}$ using quadratic optimizations, we can compute the $\\hat{\\mathbf{w}}$ and $\\hat{b}$ that minimize the primal problem using the following equations\n",
        "$$\n",
        "\\hat{\\mathbf{w}} = \\sum^m_{i=1}\\hat{\\alpha}^{(i)}t^{(i)}\\mathbf{x}^{(i)} \\\\\n",
        "\\hat{b} = \\frac{1}{n_s} \\sum^m_{\\underset{\\hat{\\alpha}^{(i)}> 0}{i=1}} t^{(i)} - \\hat{\\mathbf{w}}^T\\mathbf{x}^{(i)}\n",
        "$$\n",
        "\n",
        "where $n_s$ is the number of support vectors.\n",
        "\n",
        "The dual problem makes the kernel trick possible, while the primal one does not.\n",
        "\n",
        "### 7.4.4 Kernelized SVMs\n",
        "Suppose we want to apply second degree polynomial transformation to a 2D training set. The second degree polynomial mapping is \n",
        "$$\n",
        "\\phi(\\mathbf{x}) = \\phi\\left(\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\right) = \\begin{pmatrix} x_1^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ x_2^2 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "This is what we normally would do to transform a 2D into 3D, but if we were to take vectors $\\mathbf{a}$ and $\\mathbf{b}$, and apply this second degree polynomial mapping and compute the dot product of the transformed vectors.\n",
        "\n",
        "$$\n",
        "\\phi(\\mathbf{a})^T \\phi(\\mathbf{b}) = \\begin{pmatrix} a_1^2 \\\\ \\sqrt{2} a_1 a_2 \\\\ a_2^2 \\end{pmatrix}^T \\begin{pmatrix} b_1^2 \\\\ \\sqrt{2} b_1 b_2 \\\\ b_2^2 \\end{pmatrix} = a_1^2b_1^2 + 2a_1b_1a_2b_2 + a_2^2b_2^2 \\\\\n",
        "(a_1b_1 + a_2b_2)^2 = (\\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix}^T\\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix})^2 = (\\mathbf{a}^T\\mathbf{b})^2\n",
        "$$\n",
        "\n",
        "The dot product of the transformed vectors = square of the dot product of the original vectors: $\\phi(\\mathbf{a})^T \\phi(\\mathbf{b}) = (\\mathbf{a}^T\\mathbf{b})^2$.\n",
        "\n",
        "We can conclude that *kernel* is capable of computing the dot product of $\\phi(\\mathbf{a})^T \\phi(\\mathbf{b})$ based on the original vectors $\\mathbf{a}$ and $\\mathbf{b}$, without knowing about the transformation of $\\phi$.\n",
        "\n",
        "Here are a few common kernels (Géron, 2019).:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Linear:} \\quad & K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^\\top \\mathbf{b} \\\\\n",
        "\\text{Polynomial:} \\quad & K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^\\top \\mathbf{b} + r)^d \\\\\n",
        "\\text{Gaussian RBF:} \\quad & K(\\mathbf{a}, \\mathbf{b}) = \\exp\\left(-\\gamma \\|\\mathbf{a} - \\mathbf{b}\\|^2\\right) \\\\\n",
        "\\text{Sigmoid:} \\quad & K(\\mathbf{a}, \\mathbf{b}) = \\tanh(\\gamma \\mathbf{a}^\\top \\mathbf{b} + r)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## 7.3 Multi-class classification\n",
        "Multi-class classification in SVM can be done in 2 ways: one vs all, one vs one.\n",
        "### 7.3.1 One vs all\n",
        "Just like the name suggested, the samples from the class being trained is viewed as positive, while the rest is viewed as negative. Say we have classes '0', '1', and '2' in our dataset. There will be 3 models where model one is '0' vs {'1' ,'2'}; model 2 is '1' vs {'0' ,'2'}; model 3 is '2' vs {'0' ,'1'}.\n",
        "\n",
        "![picture](9.png)\n",
        "\n",
        "In the prediction phase, the predicted class is determined by the best model. \n",
        "\n",
        "### 7.3.1 One vs One\n",
        "The SVM algorithm in this case will train multiple binary classification. Say we have 3 classes (Blue, Green, Red), we would train 3 classifier: Blue vs Green, Blue vs Red, and Green vs Red. \n",
        "\n",
        "During the prediction phase, the 3 models will vote on the label with the highest occurrences.\n",
        "\n",
        "![picture](10.png)\n",
        "\n",
        "## 7.4 SVM in Python\n",
        "### 7.4.1 Linear kernel\n",
        "This is the iris dataset\n",
        "#### Data scaling\n"
      ],
      "id": "82bd30f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# We'll use the petal length and width only for this analysis\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# Place the iris data into a pandas dataframe\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "sc.fit(X_train)\n",
        "\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "plt.figure()\n",
        "iris_df.hist()\n",
        "plt.suptitle(\"Before scaling\")\n",
        "pd.DataFrame(X_train_std, columns=iris_df.columns).hist()\n",
        "plt.suptitle(\"After scaling\")\n",
        "plt.show()"
      ],
      "id": "3dc6f535",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### One vs rest\n"
      ],
      "id": "3fa0a44e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mlxtend.plotting import plot_decision_regions\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pca=PCA(n_components=2)\n",
        "reduced_data_train=pca.fit_transform(X_train_std)\n",
        "reduced_data_test=pca.transform(X_test_std)\n",
        "classifier=SVC(kernel='linear', random_state=0, gamma=.10, C=1.0).fit(reduced_data_train,y_train)\n",
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "classifier=SVC(kernel=\"linear\", random_state=0, gamma=.10, C=1.0,decision_function_shape='ovr').fit(reduced_data_train,y_train)\n",
        "# Plot decision boundary\n",
        "plot_decision_regions(reduced_data_test, y_test, clf=classifier, legend=2)\n",
        "plt.legend(ncol=3)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "id": "981e4195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### One vs One\n"
      ],
      "id": "60f560c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mlxtend.plotting import plot_decision_regions\n",
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "classifier=SVC(kernel=\"linear\", random_state=0, gamma=.10, C=1.0,decision_function_shape='ovo').fit(reduced_data_train,y_train)\n",
        "# Plot decision boundary\n",
        "plot_decision_regions(reduced_data_test, y_test, clf=classifier, legend=2)\n",
        "plt.legend(ncol=3)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "id": "dae78c68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4.2 Kernel trick\n",
        "#### Regularisation \n"
      ],
      "id": "1fe0b242"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from cuml.svm import SVC\n",
        "from cuml.model_selection import GridSearchCV\n",
        "\n",
        "# parameters to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],          # Regularization parameter\n",
        "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
        "    'degree': [2, 3, 4],             # Degree for the polynomial kernel\n",
        "    'gamma': ['scale', 'auto'],      # Kernel coefficient\n",
        "}\n",
        "svc = SVC()\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display the best hyperparameters\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "id": "649559d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that with grid search, we found the best hyperparameters and score of our dataset.\n",
        "\n",
        "# Week 8: Nonlinear models (KNN and DT)\n",
        "## 8.1 K-Nearest Neighbors (KNN)\n",
        "The information below is summarized from IBM (2024). \n",
        "People often mistake KNN with unsupervised learning, KNN is in fact an **supervised learning** algorithm, and can be used both for regression and classification, but mostly classification.  \n",
        "\n",
        "### Classification problem\n",
        "The class label is assigned based on the majority of the given data points surrounding it. \n",
        "\n",
        "![picture](11.png)\n",
        "\n",
        "### Regression problem\n",
        "Regression uses a similar concepts to classification, but regression average the k nearest neighbors to make prediction. One is continuous value, the other is discrete value.  \n",
        "\n",
        "Say $K=5$, and the 5 closest neighbors are 4.2, 3.8, 5.0, 4.7, 4.5. The prediction will be $\\frac{4.2+3.8+5.0+4.7+4.5}{5}=4.44$.\n",
        "\n",
        "### Compute Distance\n",
        "Before the algorithm classifies things, it needs to compute the distance. In other words, there needs to be some metrics like Euclidean or Manhattan or whatever to measure how close the other data points are to our query point (IBM, 2024).\n",
        "\n",
        "### Best number of neighbours (K)\n",
        "We need to choose the most optimal number of $K$ in order for our classification to perform well. Choosing lower number of $k$, meaning focusing on the close region, this has the higher change of overfitting. The opposite can be said for high $k$, this can cause underfitting. We can use grid search to search for the optimal number of $k$.\n",
        "\n",
        "![picture](12.png)\n",
        "\n",
        "## 8.2 Decision trees\n",
        "The information below is from Géron, A. (2019).\n",
        "\n",
        "The text discusses the training, visualizing, and making predictions with Decision Trees. Then the CART training algorithm, and how to regularize trees for regression tasks. Finally is on the limitations of Decision Trees.\n",
        "\n",
        "### Training and Visualizing a Decision Tree\n"
      ],
      "id": "fcaa6209"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X, y)"
      ],
      "id": "07593c8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(\n",
        "        tree_clf,\n",
        "        out_file=\"iris_tree.dot\",\n",
        "        feature_names=iris.feature_names[2:],\n",
        "        class_names=iris.target_names,\n",
        "        rounded=True,\n",
        "        filled=True\n",
        "    )"
      ],
      "id": "4e16a3ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![picture](iris_tree.png)\n",
        "\n",
        "### Making prediction\n",
        "Suppose there is a iris flower, and the algorithm wants to classify it. Start off with the *root node* (depth 0, top node): this node asks if $\\text{petal length} \\leq 2.45$, if yes then move down the leaf node (node that doesn't have child node) or the root's left child node (depth 1, left), which classifies the iris flower as `setosa`. \n",
        "\n",
        "If say find another flower, but this time $\\text{petal length} > 2.45$, the algorithm goes down to right child node (depth 1, right), which is not a leaf node. This right node asks $\\text{petal width} \\leq 1.75$? If yes then the class is `versicolor`, no then `virginica`.\n",
        "\n",
        "A node's `sample` tells us how many training instaces it applies to. Right child node (depth 1) has 100 instances where $\\text{petal length} > 2.45$. In those 100, 54 has a $\\text{petal width} < 1.75$.\n",
        "\n",
        "The `value` shows the classification of training instaces. Take the botton right node, there are 0 *setosa*, 1 *versicolor*, and 45 *virginica*. It made 1 mistake. The `gini` measures the purity of a node, with 0 being the purest. The formula is \n",
        "$$\n",
        "G_i = 1 - \\sum_{k=1}^np_{i,k^2}\n",
        "$$\n",
        "where $p_{i,k^2}$ is the ratio of class $k$ instances among the training instances in $i^{th}$ node.\n",
        "\n",
        "We can calculate the purity for bottom left node: $1-(0/46)^2 - (1/46)^2 - (45/46)^2 = 0.0425 = 0.043$\n",
        "\n",
        "Note: sklearn uses **CART** algorithm, whcih produces only binary trees. (yes/no answer), however algorithm like **ID3** produces nodes that have more than 2 children.\n",
        "\n",
        "This picture represents the decision tree's boundaries\n",
        "\n",
        "![picture](13.png)\n",
        "\n",
        "On the left hand where depth 0 shows a thick line because the *gini* is pure. For the right hand side where the *gini* is impure, so there are more splits.\n",
        "Since we can clearly intepret the decisions from Decision Trees, we can say this model is **white box model**. Models like Random Forest and NN are **black box models** because those models are hard to intepret the reasons for their predictions.\n",
        "\n",
        "### Estimating Class Probabilities\n",
        "A Decision Tree can estimate the probability of what class an instance belongs to by traversing the tree to the find leaf node for that instance. For example, take a flower where petals are 5cm long and 1.5cm wide. Looking at the decision boundary, we can see that this flower is versicolor. It belongs to depth 1 where the probabilities for Iris setosa (0/54) is 0%, Iris versicolor (49/54) is 90.7%, and Iris virginica (5/54) is 9.3%. \n"
      ],
      "id": "3ac1c205"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(tree_clf.predict_proba([[5, 1.5]]))\n",
        "display(tree_clf.predict([[5, 1.5]]))"
      ],
      "id": "b6a88ae6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take another example with petals 6cm long and 2cm wide, it would be virginica.\n"
      ],
      "id": "e0e7210c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(tree_clf.predict_proba([[6, 2]]))\n",
        "display(tree_clf.predict([[6, 2]]))"
      ],
      "id": "b8bb366a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The CART Training Algorithm\n",
        "Classification and Regression Tree or CART algorithm train Decision Trees (aka \"growing\" trees). The algorithm works by splitting into two subsets using a single feature $k$ and a threshold $t_k$ ($\\text{petal length} \\leq 2.45$). To choose the pair ($k$ and $t_k$), it searches for the pair that gives the purest subsets. This process repeats itself until it reaches the maximum depth (hyperparameter `max_depth`), or can not find a split to reduce the impurity. There are a few other hyperparameters. The formula for CART with the cost function that the algorithm tries to minimize is:\n",
        "$$\n",
        "J(k,t_k) = \\frac{m_{\\text{left}}}{m}G_{\\text{left}} + \\frac{m_{\\text{right}}}{m}G_{\\text{right}}\n",
        "$$\n",
        "where \n",
        "- $G_{\\text{left/right}}$ measures the impurity of the left/right subset\n",
        "- $m_{\\text{left/right}}$ is the number of instances in the left/right subset\n",
        "\n",
        "This is a greedy algorithm where it searches for the lowest possible impurity split from top down, which can lead to overfitting. \n",
        "\n",
        "### Gini Impurity or Entropy\n",
        "Impurity and Entropy are two measurement for Gini. A set's entropy is zero when it contains instances of only one class. The formula for entropy is:\n",
        "$$\n",
        "H_i = -\\sum^n_{k=1, p_{i,k^{\\neq 0}}}p_{i,k}log_2(p_{i,k})\n",
        "$$\n",
        "In depth 2, the entropy is $–(49/54) log2 (49/54) – (5/54) log2 (5/54) ≈ 0.445$.\n",
        "\n",
        "Choosing between one of the 2 is not really imporant.\n",
        "\n",
        "### Regularization Hyperparameters\n",
        "Decision Tree is a *nonparametric model*, which means there are less default parameters determined, and we need to define them. Unlike *parametric model* like Linear Regression where there are many default parameters. Since Decision Tree is a nonparametric model, and the model is trying to find the lowest possible impurity split from top down, this can cause overfitting.\n",
        "\n",
        "Hyperparameters are regularization: `max_depth` means max split. In DecisionTreeClassifier we have `min_samples_split` = minimum number of samples a node must have before split; `min_samples_leaf` = minimum number of samples a leaf must have; `min_weight_fraction_leaf` = same as `min_samples_leaf` but expressed as fraction of the total number of weighted instaces. More on the website>\n",
        "\n",
        "### Regression\n"
      ],
      "id": "fc910fe2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg.fit(X, y)"
      ],
      "id": "8a437ba0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![picture](14.png)\n",
        "\n",
        "Now instead of predicting a class for each node, it predicts a value. Say a instance $x_1=0.6$, and transverse starting from the root node, we reach a leaf node that predicts `value=0.111`. This prediction is the average target value of 110 `samples` in the leaf node, and the `mse` is 0.015 over the 110 instances.\n",
        "\n",
        "The predicted value for each region is always the average target value of the instances in that region. \n",
        "\n",
        "![picture](15.png)\n",
        "\n",
        "The CART algorithm minimizes MSE instead of purity. The formula is:\n",
        "$$\n",
        "J(k,t_k) = \\frac{m_{\\text{left}}}{m}\\text{MSE}_{\\text{left}} + \\frac{m_{\\text{right}}}{m}\\text{MSE}_{\\text{right}}\n",
        "$$\n",
        "where \n",
        "- $\\text{MSE}_{\\text{node}} = \\sum_{i \\in \\text{node}}(\\hat{y}_{\\text{node}} - y^{(i)})^2$\n",
        "- $\\hat{y}_{\\text{node}} = \\frac{1}{m_{\\text{node}}}\\sum_{i \\in \\text{node}}y^{(i)}$\n",
        "\n",
        "Just like classification, regression can overfit if no restriction is put into place.\n",
        "\n",
        "![picture](16.png)\n",
        "\n",
        "### Instability\n",
        "One of the thing is that Decision Tree loves orthogonal decision boundaries (all splits are perpendicular to an axis). This makes the boundary unnecessarily convoluted and may not generalize well.\n",
        "\n",
        "![picture](17.png)\n",
        "\n",
        "Another thing is that Decision Tree is very sensitive to small variations in the training data. If we were to remove some extreme value, the model will look vastly different. Even with the same training set, the output decision boundary can be different unless we set `random_state`. This can be solve using **Random Forest** (Géron, 2019).\n",
        "\n",
        "### Post and Pre-Pruning\n",
        "The information below is from Anand (2020).\n",
        "\n",
        "Pruning just means to remove braches of the decision tree to overcome overfitting. This can be achieved by post and pre-pruning. \n",
        "\n",
        "#### Post Pruning (Backward pruning)\n",
        "This technique is used after constructing the decision tree. We will control the `max_depth` and `min_samples_split` using `cost_complexity_pruning`. We will be using the breast_cancer dataset.\n"
      ],
      "id": "6338a96e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "id": "82301372",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X,y=load_breast_cancer(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
        "clf=DecisionTreeClassifier(random_state=0)\n",
        "clf.fit(X_train,y_train)\n",
        "y_train_predicted=clf.predict(X_train)\n",
        "y_test_predicted=clf.predict(X_test)\n",
        "\n",
        "display(\"Training accuracy:\", accuracy_score(y_train,y_train_predicted))\n",
        "display(\"Testing accuracy:\", accuracy_score(y_test,y_test_predicted))"
      ],
      "id": "c8481c56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Visualizing Decision Tree\n"
      ],
      "id": "94315eeb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "tree.plot_tree(clf)\n",
        "plt.show()"
      ],
      "id": "68f1b0f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Post-Pruning operation\n",
        "Use `cost_complexity_pruning` technique to prune the branches of decision tree.\n"
      ],
      "id": "3b87299b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path=clf.cost_complexity_pruning_path(X_train,y_train)\n",
        "\n",
        "# path variable gives two things ccp_alphas and impurities\n",
        "ccp_alphas,impurities=path.ccp_alphas,path.impurities\n",
        "print(\"ccp alpha wil give list of values :\",ccp_alphas)\n",
        "print(\"***********************************************************\")\n",
        "print(\"Impurities in Decision Tree :\",impurities)"
      ],
      "id": "77e41c30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`ccp_alphas` gives the min leaf value of decision tree, each `ccp_alpha` create different classifier and choose the best out of it\n"
      ],
      "id": "80238aa3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clfs=[]   # will store all the models here\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf=DecisionTreeClassifier(random_state=0,ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train,y_train)\n",
        "    clfs.append(clf)\n",
        "print(\"Last node in Decision tree is {} and ccp_alpha for last node is {}\".format(clfs[-1].tree_.node_count,ccp_alphas[-1]))"
      ],
      "id": "a8995fda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the accuracy score for train and test set.\n"
      ],
      "id": "098e0fe8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "id": "de635de8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following the bias and variance tradeoff, we choose that have low bias (low training error), and low variance (low test error). That value is $\\text{alpha} = 0.02$.\n"
      ],
      "id": "3529db3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf=DecisionTreeClassifier(random_state=0,ccp_alpha=0.02)\n",
        "clf.fit(X_train,y_train)\n",
        "plt.figure(figsize=(12,8))\n",
        "tree.plot_tree(clf,rounded=True,filled=True)\n",
        "plt.show()"
      ],
      "id": "8c43317d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracy_score(y_test,clf.predict(X_test))"
      ],
      "id": "62ea98c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can an improvement after post pruning the decision tree.\n",
        "\n",
        "#### Pre-Pruning\n",
        "Use GridSearch to search for the most optimal hyperparameter.\n"
      ],
      "id": "a864b649"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_param={\"criterion\":[\"gini\",\"entropy\"],\n",
        "             \"splitter\":[\"best\",\"random\"],\n",
        "             \"max_depth\":range(2,50,1),\n",
        "             \"min_samples_leaf\":range(1,15,1),\n",
        "             \"min_samples_split\":range(2,20,1) \n",
        "            }\n",
        "grid_search=GridSearchCV(estimator=clf,param_grid=grid_param,cv=5,n_jobs=-1)\n",
        "grid_search.fit(X_train,y_train)"
      ],
      "id": "39a2e65e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(grid_search.best_params_)"
      ],
      "id": "97d8950e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf=DecisionTreeClassifier(criterion= 'entropy',max_depth= 8,min_samples_leaf= 3,min_samples_split= 2,splitter= 'random')\n",
        "clf.fit(X_train,y_train)\n",
        "plt.figure(figsize=(20,12))\n",
        "tree.plot_tree(clf,rounded=True,filled=True)\n",
        "plt.show()"
      ],
      "id": "bd97f7df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_predicted=clf.predict(X_test)\n",
        "accuracy_score(y_test,y_predicted)"
      ],
      "id": "1db19272",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I think its best to use pre-pruning, and the post-pruning for the best results.\n",
        "\n",
        "### Feature Importance using DT\n"
      ],
      "id": "64cf9127"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "feature_importances = clf.feature_importances_\n",
        "sorted_indices = feature_importances.argsort()[::-1]\n",
        "sorted_importances = feature_importances[sorted_indices]\n",
        "sorted_importances"
      ],
      "id": "9c26ae63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.3 KNN in Python\n",
        "We will be using the Iris dataset\n"
      ],
      "id": "411bf146"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the Iris data set \n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# divide this data into features and labels\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print (\"X is of type: {}\".format(type(X)))\n",
        "print (\"y is of type: {}\".format(type(y)))\n",
        "\n",
        "# How does our data look\n",
        "#print first 5 rows of X\n",
        "print (\"First 5 rows of our data: {}\".format(X[:5,:]))\n",
        "\n",
        "#print the unique labels in y\n",
        "print (\"unique labels: {}\".format(np.unique(y)))"
      ],
      "id": "12f4eb5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will drop 2 columns of X for easy visualisation \n"
      ],
      "id": "15f5017e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = X[:,:2] # Use only the first 2 columns. This is for easy plotting/visualisation\n",
        "#print first 5 rows of X\n",
        "print (\"First 5 rows of our data: {}\".format(X[:5,:]))"
      ],
      "id": "2a389c1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Split the data into 80% Training and 20% Testing sets\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "\n",
        "print (Xtrain.shape)\n",
        "print (ytrain.shape)\n",
        "print (Xtest.shape)\n",
        "print (ytest.shape)\n",
        "\n",
        "Xtrain[:5,:] # first 5 rows of training data"
      ],
      "id": "5e9cca5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to plot the true data points and the calculated decision boundaries of a given classifier model. Note that this visualization is for 2D and having 3 labels or less.\n"
      ],
      "id": "bc3afebb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# We define a colormap with three colors, for three labels our data\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "\n",
        "def plot_estimator(estimator, X, y):\n",
        "    '''\n",
        "    This function takes a model (estimator), \n",
        "    '''\n",
        "    estimator.fit(X, y)\n",
        "    # Determine the maximum and minimum mesh as a boundary\n",
        "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
        "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
        "    # Generating the points on the mesh\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "    # Make predictions on the grid points\n",
        "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # for color\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Original training sample\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
        "    plt.axis('tight')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "120b2703",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# Build a kNN using 5 neighbor nodes\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "#Fit the model using our training data\n",
        "knn_model.fit(Xtrain, ytrain)\n",
        "\n",
        "# Training Accuracy:\n",
        "knn_acc = metrics.accuracy_score(ytrain, knn_model.predict(Xtrain))\n",
        "print (\"KNN Training Accuracy: {}\".format(knn_acc))\n",
        "\n",
        "#Testing Accuracy:\n",
        "knn_acc_test = metrics.accuracy_score(ytest, knn_model.predict(Xtest))\n",
        "print (\"KNN Testing Accuracy: {}\".format(knn_acc_test))\n",
        "\n",
        "# Visualize the decision bounday. The points represent the true data. \n",
        "plot_estimator(knn_model, Xtrain, ytrain)"
      ],
      "id": "c6445ff8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.4 Decision Trees in Python\n",
        "We will use the Titanic model to predicts if a passenger survived or not.\n"
      ],
      "id": "3c9b21c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()"
      ],
      "id": "f9c1da32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv('titanic_train.csv')\n",
        "df = df[['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'Embarked']]\n",
        "df['Sex'] = le.fit_transform(df['Sex'])\n",
        "df['Embarked'] = le.fit_transform(df['Embarked'])\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df"
      ],
      "id": "980147c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split data\n",
        "X = df.drop('Survived' ,axis=1)\n",
        "y = df.Survived\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "id": "ff5189b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pre-pruning\n"
      ],
      "id": "64af227c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_param={\"criterion\":[\"gini\",\"entropy\"],\n",
        "             \"splitter\":[\"best\",\"random\"],\n",
        "             \"max_depth\":range(2,50,1),\n",
        "             \"min_samples_leaf\":range(1,15,1),\n",
        "             \"min_samples_split\":range(2,20,1) \n",
        "            }\n",
        "grid_search=GridSearchCV(estimator=clf,param_grid=grid_param,cv=5,n_jobs=-1)\n",
        "grid_search.fit(X_train,y_train)"
      ],
      "id": "5bc10ea9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(grid_search.best_params_)"
      ],
      "id": "422226a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf=DecisionTreeClassifier(criterion= 'entropy',max_depth= 48,min_samples_leaf= 2,min_samples_split= 13,splitter= 'random')\n",
        "clf.fit(X_train,y_train)\n",
        "plt.figure(figsize=(20,12))\n",
        "tree.plot_tree(clf,rounded=True,filled=True)\n",
        "plt.show()"
      ],
      "id": "c11a329c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_predicted=clf.predict(X_test)\n",
        "accuracy_score(y_test,y_predicted)"
      ],
      "id": "50633117",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Post-pruning\n"
      ],
      "id": "1c430a98"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path=clf.cost_complexity_pruning_path(X_train,y_train)\n",
        "\n",
        "# path variable gives two things ccp_alphas and impurities\n",
        "ccp_alphas,impurities=path.ccp_alphas,path.impurities\n",
        "print(\"ccp alpha wil give list of values :\",ccp_alphas)\n",
        "print(\"***********************************************************\")\n",
        "print(\"Impurities in Decision Tree :\",impurities)"
      ],
      "id": "4a85e6d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clfs=[]   # will store all the models here\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf=DecisionTreeClassifier(random_state=0,ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train,y_train)\n",
        "    clfs.append(clf)\n",
        "print(\"Last node in Decision tree is {} and ccp_alpha for last node is {}\".format(clfs[-1].tree_.node_count,ccp_alphas[-1]))"
      ],
      "id": "b446d526",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "id": "389d5d67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf=DecisionTreeClassifier(random_state=0,ccp_alpha=0.02)\n",
        "clf.fit(X_train,y_train)\n",
        "plt.figure(figsize=(12,8))\n",
        "tree.plot_tree(clf,rounded=True,filled=True)\n",
        "plt.show()\n",
        "accuracy_score(y_test,clf.predict(X_test))"
      ],
      "id": "97920f9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n",
        "Anand, A. (2020). Post-Pruning and Pre-Pruning in Decision Tree. [online] Medium. Available at: https://medium.com/analytics-vidhya/post-pruning-and-pre-pruning-in-decision-tree-561f3df73e65.\n",
        "\n",
        "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition [Book]. [online] www.oreilly.com. Available at: https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/.\n",
        "\n",
        "IBM (2024). What is the k-nearest neighbors algorithm? | IBM. [online] www.ibm.com. Available at: https://www.ibm.com/topics/knn#:~:text=The%20k%2Dnearest%20neighbors%20(KNN.\n"
      ],
      "id": "657a1000"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}