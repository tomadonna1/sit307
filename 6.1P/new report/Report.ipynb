{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning and Linear models\n",
    "I will cite external resources, Deakin resources will not be cited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5: Fundamentals of supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Concept\n",
    "The information below is from the Machine Learning Specialization (Ng, A., n.d.) course.\n",
    "\n",
    "Linear regression maps input data $\\mathbf{x}$ to labelled data $\\mathbf{y}$. The formula is:\n",
    "$$\n",
    "f_{\\mathbf{w}, b}(\\mathbf{x}) = \\mathbf{w}\\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Our goal is the find the best $w$ weights and $b$ bias. \n",
    "\n",
    "The cost function $J(\\mathbf{w},b)$ will help use choose the best weights and bias. To find the smallest cost, we need **gradient descent**. \n",
    "\n",
    "Note: $\\mathbf{w}$ is a vector or column matrix $\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}$ and $b$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Descent\n",
    "The cost function formula is\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})^2 = J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b - \\mathbf{y}^{(i)})^2$$\n",
    "\n",
    "where $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalar or float to support multiple features; $m$ is the number of training data.\n",
    "\n",
    "Before I introduced to gradient descent, we need to compute the gradient, which is the final form of the partial derivative of weights and bias.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\mathbf{x}^{(i) \\times} {\\mathbf{w}} + b - y^{(i)})x_{j}^{(i)} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\mathbf{x}^{(i)} \\times {\\mathbf{w}} + b - y^{(i)})\n",
    "\\end{align}\n",
    "$$\n",
    "* $m$ is the number of training examples in the data set\n",
    "* $\\mathbf{w}$ is weight vector\n",
    "* $\\mathbf{x}^{(i)}$ and $\\mathbf{y}^{(i)}$ are vectors as well\n",
    "\n",
    "So here is the formula for gradient descent\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{1}  \\; & \n",
    "\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, determining how fast the gradient descent converge, or find the optimal parameters. We don't want to set it too high or too small, coz the model might not converge and converge very slowly (Ng, A., n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Square Error\n",
    "This metric measures how close the predictions are to the true target values. Mean Square Error or MSE formula is:\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^n(\\mathbf{y}_i - \\mathbf{\\hat{y}}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014776657456842828"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(predictions: np.ndarray, target_values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the MSE between predictions and target values\n",
    "\n",
    "    Args:\n",
    "        prediction (np.ndarray): _description_\n",
    "        target_value (np.ndarray): _description_\n",
    "        \n",
    "    Returns:\n",
    "    float: mse\n",
    "    \"\"\"\n",
    "    mse = np.mean((predictions - target_values)**2)/len(predictions)\n",
    "    return mse\n",
    "\n",
    "mean_square_error = mse(result_df['prediction'], result_df['target_value'])\n",
    "mean_square_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>15.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.283892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>19.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       Radio   Newspaper       Sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   15.130500\n",
       "std     85.854236   14.846809   21.778621    5.283892\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   11.000000\n",
       "50%    149.750000   22.900000   25.750000   16.000000\n",
       "75%    218.825000   36.525000   45.100000   19.050000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Sales is in the range of 1.6 to 27 (measures thousands of units), with an std of 5.28. The mse (the average of the squared difference between the actual and predicted sales) of our model is 0.01477. This means that the mse is relatively small compared to the overall variance in the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSE (Root Mean Square Error)\n",
    "Root Mean Square Error derive from MSE and its computed as:\n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12155927548666465"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_square_error)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of 0.12 shows on average, the model's predictions are off by 1.12 thousand units in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAE (Mean Absolute Error)\n",
    "Mean Absolute Error is similar to MSE and RMSE, a popular metric that is robust to outliers. The formula is\n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n|\\mathbf{y}_i - \\mathbf{\\hat{y}}_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01655921894249485"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mae(predictions: np.ndarray, target_values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the MSE between predictions and target values\n",
    "\n",
    "    Args:\n",
    "        prediction (np.ndarray): _description_\n",
    "        target_value (np.ndarray): _description_\n",
    "        \n",
    "    Returns:\n",
    "    float: mae\n",
    "    \"\"\"\n",
    "    mae = (1/len(predictions)) * np.sum((predictions - target_values))\n",
    "    return mae\n",
    "     \n",
    "    \n",
    "mae(result_df['prediction'], result_df['target_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.02 MAE, very small, which shows our model is performing very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explained Variance ($R^2$)\n",
    "Explained Variance, R-square, the coefficient of determination, is just the measure of the percentage of target variation that is explained by the model. In other words, it is the square correlation between the target values and the predicted target values.\n",
    "$$\n",
    "R^2 = \\frac{\\text{Variance Explained by the model}}{\\text{Total Variance}}\n",
    "$$\n",
    "Unlike the other metrics, the higher the $R^2$, the better the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9231773833754602"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r_squared(predictions: np.ndarray, target_values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the R-squared between predictions and target values.\n",
    "\n",
    "    Args:\n",
    "        predictions (np.ndarray): Predicted values.\n",
    "        target_values (np.ndarray): Actual target values.\n",
    "        \n",
    "    Returns:\n",
    "    float: R-squared value.\n",
    "    \"\"\"\n",
    "    ss_total = np.sum((target_values - np.mean(target_value))**2)\n",
    "    ss_residual = np.sum((target_values - predictions)**2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2\n",
    "\n",
    "r_squared(result_df['prediction'], result_df['target_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gives a $R^2$ value of 0.923 which is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Classification\n",
    "Classification is similar to regression, but instead of predicting continuous data, it predict discrete data.\n",
    "\n",
    "There are 2 types of classification: binary and multi-class. In binary classification, we introduce a function called sigmoid function. The formula for sigmoid function is\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "The formula is not much different from regression formula, with the addition of $g$ which is the sigmoid function. The point of the sigmoid function is to transform the continuous output in a range between 0 and 1. The sigmoid function is defined as:\n",
    "$$g(\\mathbf{w}\\cdot \\mathbf{x} + b) = g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "The cost function for binary classification is called log-loss, unlike square error from regression. It is defined as:\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] $$\n",
    "where\n",
    "- $m$ is the number of training data\n",
    "- $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ or $loss(-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$ is the cost for a single data point.\n",
    "    - Recall that $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is $g(z)$ or model's prediction, and $y^{(i)}$ is the labels.\n",
    "\n",
    "Just remember that lower log loss means the model performs better.\n",
    "\n",
    "The iteration of gradient descent is similar to Linear Regression. The output of Logistic Regression is in probability, we can set if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$, if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix\n",
    "Confusion matrix summarizes the prediction results via the number of correct and incorrect predictions with count values of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[116,   9],\n",
       "       [ 26,  41]], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix provides real performance of a classifier because it shows us the correct and incorrect count values of each label, meaning if our dataset is unbalanced, we will know that, unlike with accuracy where we do not know whether our data is unbalanced or not.\n",
    "\n",
    "Here is what the output mean:\n",
    "- The <span style=\"color:red\">diagonal values</span> represent the elements where the **predicted classes** = **actual classes**.\n",
    "- The <span style=\"color:red\">off-diagonal values</span> represent wrong predictions.\n",
    "\n",
    "![picture](1.png){width=550 height=300}\n",
    "\n",
    "This picture shows the concept clearly, with **positive class** as **class 1** and **negative class** as **class 0**. There a few formulas derived from this.\n",
    "\n",
    "- **Accuracy**: like I said already, accuracy is not useful for imbalance class problems, and we may need to use other metrics, but its a start.\n",
    "$$\n",
    "\\text{accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "$$\n",
    "\n",
    "- **True Positive Rate** (TPR) or Recall or Sensitivity: correctly predicted positive. In other words, the fraction of TP over amount of positive samples (TP + FN).\n",
    "$$\n",
    "\\text{recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **False positive rate** (FPR): wrongly predicted positive. In other words, the fraction of FP over the amount of negative sample (FP + TN).\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "An article on DataCamp from Navlani, A. (2019) provides us with more metrics.\n",
    "\n",
    "- **Precision**: ratio of correctly predicted positive or TP over the total predicted positives (TP + FP).\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **F1**: combine precision and recall into a single metric, useful when class distribution is imbalanced. \n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- **Support**: the number of actual occurrences of each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " no diabetes       0.82      0.93      0.87       125\n",
      "    diabetes       0.82      0.61      0.70        67\n",
      "\n",
      "    accuracy                           0.82       192\n",
      "   macro avg       0.82      0.77      0.78       192\n",
      "weighted avg       0.82      0.82      0.81       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['no diabetes', 'diabetes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that \n",
    "- The model perform well with 0.82 accuracy.\n",
    "- The model does better at predicting \"no diabetes\" (high recall of 0.93) compare to \"diabetes\" cases (lower recall of 0.61).\n",
    "- F1-score of 0.7 for \"diabetes\" meaning the model performance for this label is weaker, which can be a concern if correctly identifying diabetes is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve\n",
    "Receiver Operating Characteristic (ROC) curve is a plot that plot the TPR again the FPR. It shows us the trade-offs between benefits (TP) and costs (FP). This depends on the problem we are trying to solve, like how much can we afford FPR (Navlani, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz10lEQVR4nO3de1jUdd7/8deAzCAJiCl4CPFQWXbQ0mTRXLdiZVsvy7231UtLyc7l7l3xW1PXA2Wtuh1cu4vWO/O0uxkd7qzuNMko7CrZNUm2Wk3zlG4K6m4CggIO798f3cw6Acog8GXg+biuuS75zuf7nc98BmdefL7v72dcZmYCAABwSIjTHQAAAG0bYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFGEEQAA4Kh2TnegPqqqqnTgwAFFRkbK5XI53R0AAFAPZqaSkhJ1795dISF1z38ERRg5cOCA4uPjne4GAABogP379+u8886r8/6gCCORkZGSvnsyUVFRDvcGAADUR3FxseLj432f43UJijBSfWomKiqKMAIAQJA5U4kFBawAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFEBh5EPP/xQo0ePVvfu3eVyufTGG2+ccZ+cnBxdeeWV8ng8Ov/887VixYoGdBUAALRGAYeR0tJSDRgwQBkZGfVqv2fPHo0aNUrXXHON8vPz9cADD+iOO+5QVlZWwJ0FAACtT8DfTXP99dfr+uuvr3f7xYsXq3fv3nrqqackSRdffLE++ugj/f73v1dKSkqgDw8AOA0z0/FKr9PdQBBqHxZ6xu+QaSpN/kV5ubm5Sk5O9tuWkpKiBx54oM59ysvLVV5e7vu5uLi4qboHAK2GmemmxbnK+/pbp7uCILR1booi3M58f26TF7AWFBQoLi7Ob1tcXJyKi4t1/PjxWveZP3++oqOjfbf4+Pim7iYABL3jlV6CCIKSMxHoDGbMmKG0tDTfz8XFxQQSAAjA5lnJinCHOt0NBJH2Yc79vjR5GOnatasKCwv9thUWFioqKkrt27evdR+PxyOPx9PUXQOAVivCHerYlDsQqCb/TU1KStLatWv9tq1fv15JSUlN/dAA4LjmLCgtq6BwFcEp4DBy7Ngx7dy50/fznj17lJ+fr06dOqlnz56aMWOGvvnmG/3xj3+UJN1zzz169tln9dBDD+m2227T+++/r1deeUVr1qxpvGcBAC0QBaVA/QRcwLp582ZdccUVuuKKKyRJaWlpuuKKKzRnzhxJ0sGDB7Vv3z5f+969e2vNmjVav369BgwYoKeeekovvPACl/UCaPWcKigdnBDj6Pl/IFAuMzOnO3EmxcXFio6OVlFRkaKiopzuDgDUS1nFSfWf890Cj81ZUOrkehHAqer7+U11EwA0AwpKgbrxRXkAAMBRxHQArZbTS6NzdQtQP4QRAK0SV7IAwYPTNABapZa0NDpXtwCnx8wIgFbP6aXRuboFOD3CCIBWjytZgJaN/50AWpzGKDyleBQIHoQRAC0KhadA20MBK4AWpbELTykeBVo+ZkYAtFiNUXhK8SjQ8hFGALRYFJ4CbQP/ywE0mYYUolJ4CrQ9hBEATYJCVAD1RQErgCZxtoWoFJ4CbQczIwCaXEMKUSk8BdoOwgiAJkchKoDT4TQNAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHUd4OtEENWRk1UKykCqC+CCNAG8PKqABaGk7TAG3M2a6MGihWUgVwJsyMAG1YQ1ZGDRQrqQI4E8II0IaxMiqAloB3ISCINEbhKYWlAFoawggQJCg8BdBaUcAKBInGLjylsBRAS8HMCBCEGqPwlMJSAC0FYQQIQhSeAmhNeDcDWoD6FKZSeAqgtSKMAA6jMBVAW0cBK+CwQAtTKTwF0NowMwK0IPUpTKXwFEBrQxgBWhAKUwG0RbzrAQFojBVQv4/CVABtHWEEqCcKTQGgaVDACtRTY6+A+n0UpgJoq5gZARqgMVZA/T4KUwG0VYQRoAEoNAWAxsNpGgAA4CjCCAAAcBRhBAAAOIowAgAAHEUYAQAAjuJyALRKrJQKAMGDMIJWh5VSASC4cJoGrQ4rpQJAcGFmBK0aK6UCQMtHGEGrxkqpANDy8S6NVqO6aJVCUwAILoQRtAoUrQJA8KKAFa1CbUWrFJoCQHBgZgStTnXRKoWmABAcCCNodShaBYDg0qDTNBkZGerVq5fCw8OVmJioTZs2nbb9okWL1K9fP7Vv317x8fF68MEHdeLEiQZ1GG2Xmams4mQdN4pWASBYBfzn48svv6y0tDQtXrxYiYmJWrRokVJSUrR9+3bFxsbWaL9q1SpNnz5dy5Yt09ChQ7Vjxw7deuutcrlcWrhwYaM8CbR+FKgCQOsV8MzIwoULdeedd2ry5Mnq37+/Fi9erIiICC1btqzW9hs3btSwYcM0YcIE9erVSyNHjtT48ePPOJsCnKq+q6pStAoAwSegmZGKigrl5eVpxowZvm0hISFKTk5Wbm5urfsMHTpUf/7zn7Vp0yYNGTJEu3fv1tq1azVx4sQ6H6e8vFzl5eW+n4uLiwPpJlq5062qStEqAASfgMLIkSNH5PV6FRcX57c9Li5OX375Za37TJgwQUeOHNHVV18tM9PJkyd1zz336De/+U2djzN//nw98sgjgXQNbQgFqgDQujT5OiM5OTmaN2+ennvuOX366ad6/fXXtWbNGj366KN17jNjxgwVFRX5bvv372/qbqKF+nfRKgWqANBaBfTnZefOnRUaGqrCwkK/7YWFheratWut+8yePVsTJ07UHXfcIUm67LLLVFpaqrvuukszZ85USEjNPOTxeOTxeALpGlohilYBoG0IaGbE7XZr0KBBys7O9m2rqqpSdna2kpKSat2nrKysRuAIDf3ufL+ZBdpftCGsqgoAbUPAJ97T0tKUmpqqwYMHa8iQIVq0aJFKS0s1efJkSdKkSZPUo0cPzZ8/X5I0evRoLVy4UFdccYUSExO1c+dOzZ49W6NHj/aFEuBMWFUVAFqvgMPIuHHjdPjwYc2ZM0cFBQUaOHCg1q1b5ytq3bdvn99MyKxZs+RyuTRr1ix988036tKli0aPHq3f/va3jfcs0OpRtAoArZfLguBcSXFxsaKjo1VUVKSoqCinu4MAmZmOVwZegFpW4dXgx96TJG2dm0IYAYAgU9/Pb97d0aQoQgUAnEmTX9qLtq2+K6eeDkWrANC6MTOCZnO6lVNPh6JVAGjdCCNoNhShAgBqw2kaAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACO4tIGNInqVVfLKgJfeRUA0LYQRtDoWHUVABAITtOg0dW26iqrqAIA6sLMCJpU9aqrrKIKAKgLYQRNilVXAQBnwqcE6q26KPVMKFoFAASCMIJ6oSgVANBUKGBFvdRWlHomFK0CAOqDmREErLoo9UwoWgUA1AdhBAGjKBUA0Jj4REGdTi1YpSgVANBUCCOoFQWrAIDmQgEralVXwSpFqQCAxsbMCM7o1IJVilIBAI2NMIIzomAVANCUOE0DAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRXCIBP9WrrrLiKgCguRBG4MOqqwAAJ3CaBj61rbrKiqsAgKbGzAhqVb3qKiuuAgCaGmEEtWLVVQBAc+E0DQAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAAR3G5RCtUvYpqoFh1FQDgBMJIK8MqqgCAYMNpmlamtlVUA8WqqwCA5sTMSCtWvYpqoFh1FQDQnAgjrRirqAIAggGfVC1IQwtPT0URKgAg2BBGWggKTwEAbRUFrC1EYxSenooiVABAsGBmpAVqaOHpqShCBQAEC8JIC0ThKQCgLeE0DQAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAAR3HJhsOqV11l5VQAQFtFGHEQq64CANDA0zQZGRnq1auXwsPDlZiYqE2bNp22/dGjRzVlyhR169ZNHo9HF154odauXdugDrcmta26ysqpAIC2JuCZkZdffllpaWlavHixEhMTtWjRIqWkpGj79u2KjY2t0b6iokI//vGPFRsbq9dee009evTQ119/rY4dOzZG/1uN6lVXWTkVANDWBBxGFi5cqDvvvFOTJ0+WJC1evFhr1qzRsmXLNH369Brtly1bpn/961/auHGjwsLCJEm9evU6u163Qqy6CgBoqwI6TVNRUaG8vDwlJyf/+wAhIUpOTlZubm6t+7z11ltKSkrSlClTFBcXp0svvVTz5s2T11t3wWZ5ebmKi4v9bgAAoHUKKIwcOXJEXq9XcXFxftvj4uJUUFBQ6z67d+/Wa6+9Jq/Xq7Vr12r27Nl66qmn9Nhjj9X5OPPnz1d0dLTvFh8fH0g3AQBAEGnydUaqqqoUGxur559/XoMGDdK4ceM0c+ZMLV68uM59ZsyYoaKiIt9t//79Td1NAADgkICKFDp37qzQ0FAVFhb6bS8sLFTXrl1r3adbt24KCwtTaOi/rxC5+OKLVVBQoIqKCrnd7hr7eDweeTyeQLoGAACCVEAzI263W4MGDVJ2drZvW1VVlbKzs5WUlFTrPsOGDdPOnTtVVVXl27Zjxw5169at1iACAADaloBP06SlpWnJkiVauXKltm3bpnvvvVelpaW+q2smTZqkGTNm+Nrfe++9+te//qX7779fO3bs0Jo1azRv3jxNmTKl8Z5FEDEzlVWc/L8bq64CABDwtaTjxo3T4cOHNWfOHBUUFGjgwIFat26dr6h13759Cgn5d8aJj49XVlaWHnzwQV1++eXq0aOH7r//fk2bNq3xnkWQYMVVAABqcpmZOd2JMykuLlZ0dLSKiooUFRXldHcarKzipPrPyaqxfXBCjF69J4nFzgAArUp9P79ZZcsh1SuuSmLVVQBAm0YYcQgrrgIA8J0mX2cEAADgdAgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRrEfeBMxMxyu9NbaXVdTcBgBAW0cYaWRmppsW5yrv62+d7goAAEGB0zSN7Hil94xBZHBCjNqHhTZTjwAAaNmYGWlCm2clK8JdM3S0DwuVy+VyoEcAALQ8hJEmFOEOVYSbIQYA4HT4pGwEpxasUqQKAEBgCCNniYJVAADODgWsZ6muglWKVAEAqB9mRhrRqQWrFKkCAFA/hJFGRMEqAACB45OzgaqLVilYBQDg7BBGGoCiVQAAGg8FrA1QW9EqBasAADQMMyNnqbpolYJVAAAahjBylihaBQDg7HCaBgAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAo7gMpJ6qV1yVxKqrAAA0IsJIPbDiKgAATYfTNPVQ24qrEquuAgDQGJgZCVD1iquSWHUVAIBGQBgJECuuAgDQuDhNAwAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHBUO6c70JKYmY5XemtsL6uouQ0AADQOwsj/MTPdtDhXeV9/63RXAABoUzhN83+OV3rPGEQGJ8SofVhoM/UIAIC2gZmRWmyelawId83Q0T4sVC6Xy4EeAQDQejVoZiQjI0O9evVSeHi4EhMTtWnTpnrtl5mZKZfLpTFjxjTkYZtNhDtUEe52NW4EEQAAGl/AYeTll19WWlqa0tPT9emnn2rAgAFKSUnRoUOHTrvf3r179etf/1rDhw9vcGcBAEDrE3AYWbhwoe68805NnjxZ/fv31+LFixUREaFly5bVuY/X69XNN9+sRx55RH369DmrDgMAgNYloDBSUVGhvLw8JScn//sAISFKTk5Wbm5unfvNnTtXsbGxuv322+v1OOXl5SouLva7AQCA1imgMHLkyBF5vV7FxcX5bY+Li1NBQUGt+3z00UdaunSplixZUu/HmT9/vqKjo323+Pj4QLoJAACCSJNe2ltSUqKJEydqyZIl6ty5c733mzFjhoqKiny3/fv3N2EvAQCAkwK6tLdz584KDQ1VYWGh3/bCwkJ17dq1Rvtdu3Zp7969Gj16tG9bVVXVdw/crp22b9+uvn371tjP4/HI4/EE0jUAABCkApoZcbvdGjRokLKzs33bqqqqlJ2draSkpBrtL7roIn3++efKz8/33W644QZdc801ys/P5/QLAAAIfNGztLQ0paamavDgwRoyZIgWLVqk0tJSTZ48WZI0adIk9ejRQ/Pnz1d4eLguvfRSv/07duwoSTW2AwCAtingMDJu3DgdPnxYc+bMUUFBgQYOHKh169b5ilr37dunkBBWmQcAAPXjMjNzuhNnUlxcrOjoaBUVFSkqKqpJHqOs4qT6z8mSJG2dm6IINyvlAwBwNur7+c0UBgAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFGEEQAA4CjCCAAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFGEEQAA4CjCCAAAcBRhBAAAOKqd0x1wkpnpeKVXklRW4XW4NwAAtE1tNoyYmW5anKu8r791uisAALRpbfY0zfFKb61BZHBCjNqHhTrQIwAA2qY2OzNyqs2zkhXh/i6AtA8LlcvlcrhHAAC0HYQRSRHuUEW4GQoAAJzQZk/TAACAlqFBYSQjI0O9evVSeHi4EhMTtWnTpjrbLlmyRMOHD1dMTIxiYmKUnJx82vYAAKBtCTiMvPzyy0pLS1N6ero+/fRTDRgwQCkpKTp06FCt7XNycjR+/Hh98MEHys3NVXx8vEaOHKlvvvnmrDsPAACCn8vMLJAdEhMTddVVV+nZZ5+VJFVVVSk+Pl6/+tWvNH369DPu7/V6FRMTo2effVaTJk2q12MWFxcrOjpaRUVFioqKCqS7dSqrOKn+c7IkSVvnplAzAgBAI6vv53dAMyMVFRXKy8tTcnLyvw8QEqLk5GTl5ubW6xhlZWWqrKxUp06d6mxTXl6u4uJivxsAAGidAgojR44ckdfrVVxcnN/2uLg4FRQU1OsY06ZNU/fu3f0CzffNnz9f0dHRvlt8fHwg3QQAAEGkWa+mWbBggTIzM7V69WqFh4fX2W7GjBkqKiry3fbv39+MvQQAAM0poEKJzp07KzQ0VIWFhX7bCwsL1bVr19Pu++STT2rBggV67733dPnll5+2rcfjkcfjCaRrAAAgSAU0M+J2uzVo0CBlZ2f7tlVVVSk7O1tJSUl17vf444/r0Ucf1bp16zR48OCG9xYAALQ6AV9CkpaWptTUVA0ePFhDhgzRokWLVFpaqsmTJ0uSJk2apB49emj+/PmSpN/97neaM2eOVq1apV69evlqSzp06KAOHTo04lMBAADBKOAwMm7cOB0+fFhz5sxRQUGBBg4cqHXr1vmKWvft26eQkH9PuPzhD39QRUWFbrrpJr/jpKen6+GHHz673gMAgKAX8DojTmCdEQAAgk+TrDMCAADQ2AgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFGEEQAA4CjCCAAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFGEEQAA4CjCCAAAcFQ7pzsAoGG8Xq8qKyud7gaANiwsLEyhoaFnfRzCCBBkzEwFBQU6evSo010BAHXs2FFdu3aVy+Vq8DEII0CQqQ4isbGxioiIOKs3AABoKDNTWVmZDh06JEnq1q1bg49FGAGCiNfr9QWRc8891+nuAGjj2rdvL0k6dOiQYmNjG3zKhgJWIIhU14hEREQ43BMA+E71+9HZ1LARRoAgxKkZAC1FY7wfEUYAAICjCCMAHPGjH/1IDzzwgNPdANACEEYABIWcnBy5XK5GuaT5ww8/1OjRo9W9e3e5XC698cYbZ33MliInJ0dXXnmlPB6Pzj//fK1YseKM+2RlZekHP/iBIiMj1aVLF/385z/X3r17ffffeuutcrlcNW6XXHKJr80f/vAHXX755YqKilJUVJSSkpL0zjvv+D3Oj370oxrHuOeee/za1PY4mZmZvvtff/11/fjHP1aXLl18j5OVleV3jPnz5+uqq65SZGSkYmNjNWbMGG3fvt2vzd13362+ffuqffv26tKli2688UZ9+eWXfm0++eQTXXfdderYsaNiYmKUkpKiv/3tbwGNnXTm1+Thhx+u8Zwvuugivza7du3Sz372M9/zHjt2rAoLC/0eo7axc7lc+uSTT3ztPvvsMw0fPlzh4eGKj4/X448/7vc4S5Ys0fDhwxUTE6OYmBglJydr06ZNamqEEQBtTmlpqQYMGKCMjAynu9Ko9uzZo1GjRumaa65Rfn6+HnjgAd1xxx01Pqy/v8+NN96oa6+9Vvn5+crKytKRI0f0H//xH742Tz/9tA4ePOi77d+/X506ddIvfvELX5vzzjtPCxYsUF5enjZv3qxrr71WN954o/7+97/7Pd6dd97pd6zvfxhK0vLly/3ajBkzxnffhx9+qB//+Mdau3at8vLydM0112j06NHasmWLr82GDRs0ZcoU/eUvf9H69etVWVmpkSNHqrS01Ndm0KBBWr58ubZt26asrCyZmUaOHCmv1ytJOnbsmH7yk5+oZ8+e+utf/6qPPvpIkZGRSklJ8RVq1mfs6vuaXHLJJX7P+aOPPvLdV1paqpEjR8rlcun999/Xxx9/rIqKCo0ePVpVVVWSpKFDh/rtf/DgQd1xxx3q3bu3Bg8eLEkqLi7WyJEjlZCQoLy8PD3xxBN6+OGH9fzzz/seKycnR+PHj9cHH3yg3NxcxcfHa+TIkfrmm2/q+hVqHBYEioqKTJIVFRU12jFLyystYdrbljDtbSstr2y04wJN6fjx47Z161Y7fvy4010JyLFjx2zixIl2zjnnWNeuXe3JJ5+0ESNG2P333+9r88c//tEGDRpkHTp0sLi4OBs/frwVFhaamdmePXtMkt8tNTXVzMzeeecdGzZsmEVHR1unTp1s1KhRtnPnznr3TZKtXr26Qc/roYcesgsuuMDat29vvXv3tlmzZllFRYXv/tTUVLvxxhv99rn//vttxIgRvp+9Xq/97ne/s759+5rb7bb4+Hh77LHHGtyfSy65xG/buHHjLCUlpc59Xn31VWvXrp15vV7ftrfeestcLpffcznV6tWrzeVy2d69e0/bn5iYGHvhhRd8P3//Na9NQ16P/v372yOPPFLn/YcOHTJJtmHDhjrb/O1vfzNJvt+dTz75xCTZvn37fG0+++wzk2RfffWVmdVv7OrzmqSnp9uAAQPq7FtWVpaFhIT4fQYePXrUXC6XrV+/vtZ9KioqrEuXLjZ37lzftueee85iYmKsvLzct23atGnWr1+/Oh/75MmTFhkZaStXrqyzzenel+r7+c3MCBDEzExlFScduZlZvfs5depUbdiwQW+++abeffdd5eTk6NNPP/VrU1lZqUcffVR/+9vf9MYbb2jv3r269dZbJUnx8fH6n//5H0nS9u3bdfDgQT399NOSvvurMS0tTZs3b1Z2drZCQkL0s5/9zPcXY1OKjIzUihUrtHXrVj399NNasmSJfv/73wd0jBkzZmjBggWaPXu2tm7dqlWrVikuLs53/yWXXKIOHTrUebv++ut9bXNzc5WcnOx3/JSUFOXm5tb5+IMGDVJISIiWL18ur9eroqIi/elPf1JycrLCwsJq3Wfp0qVKTk5WQkJCrfd7vV5lZmaqtLRUSUlJfve9+OKL6ty5sy699FLNmDFDZWVlNfafMmWKOnfurCFDhmjZsmWn/V2rqqpSSUmJOnXqVGeboqIiSaqzTWlpqZYvX67evXsrPj5ektSvXz+de+65Wrp0qSoqKnT8+HEtXbpUF198sXr16iWpfmNX39fkq6++Uvfu3dWnTx/dfPPN2rdvn+++8vJyuVwueTwe37bw8HCFhIT4zaCc6q233tI///lPTZ482bctNzdXP/zhD+V2u/36sn37dn377be1HqesrEyVlZWnHd/G0KBFzzIyMvTEE0+ooKBAAwYM0DPPPKMhQ4bU2f7VV1/V7NmztXfvXl1wwQX63e9+p5/+9KcN7jSA7xyv9Kr/nLqn4JvS1rkpinCf+S3k2LFjWrp0qf785z/ruuuukyStXLlS5513nl+72267zffvPn366L/+67901VVX6dixY+rQoYPvzTA2NlYdO3b0tf35z3/ud5xly5apS5cu2rp1qy699NKGPr16mTVrlu/fvXr10q9//WtlZmbqoYceqtf+JSUlevrpp/Xss88qNTVVktS3b19dffXVvjZr16497foN1YtOSd+tzntqkJGkuLg4FRcX6/jx435tq/Xu3Vvvvvuuxo4dq7vvvlter1dJSUlau3ZtrY934MABvfPOO1q1alWN+z7//HMlJSXpxIkT6tChg1avXq3+/fv77p8wYYISEhLUvXt3ffbZZ5o2bZq2b9+u119/3ddm7ty5uvbaaxUREaF3331X9913n44dO6b//M//rLU/Tz75pI4dO6axY8fWen9VVZUeeOABDRs2rMbvw3PPPaeHHnpIpaWl6tevn9avX+/7oI6MjFROTo7GjBmjRx99VJJ0wQUXKCsrS+3atav32NXnNUlMTNSKFSvUr18/HTx4UI888oiGDx+uL774QpGRkfrBD36gc845R9OmTdO8efNkZpo+fbq8Xq8OHjxY6/NeunSpUlJS/P6fFRQUqHfv3jX6Un1fTExMjeNMmzZN3bt3rxGoGlvAMyMvv/yy0tLSlJ6erk8//VQDBgxQSkqKbznY79u4caPGjx+v22+/XVu2bNGYMWM0ZswYffHFF2fdeQAt365du1RRUaHExETftk6dOqlfv35+7fLy8jR69Gj17NlTkZGRGjFihCT5/YVYm6+++krjx49Xnz59FBUV5fur9Uz7NYaXX35Zw4YNU9euXdWhQwfNmjUroMfdtm2bysvLfSGtNgkJCTr//PPrvPXo0eOsnkNBQYHuvPNOpaam6pNPPtGGDRvkdrt100031TojsXLlSnXs2NGvjqNav379lJ+fr7/+9a+69957lZqaqq1bt/ruv+uuu5SSkqLLLrtMN998s/74xz9q9erV2rVrl6/N7NmzNWzYMF1xxRWaNm2aHnroIT3xxBO19n3VqlV65JFH9Morryg2NrbWNlOmTNEXX3zhVwRb7eabb9aWLVu0YcMGXXjhhRo7dqxOnDghSTp+/Lhuv/12DRs2TH/5y1/08ccf69JLL9WoUaN0/PjxBo1dXa6//nr94he/0OWXX66UlBStXbtWR48e1SuvvCJJ6tKli1599VX97//+rzp06KDo6GgdPXpUV155pUJCan6M/+Mf/1BWVpZuv/32evehNgsWLFBmZqZWr16t8PDwszrWGZ32JE4thgwZYlOmTPH97PV6rXv37jZ//vxa248dO9ZGjRrlty0xMdHuvvvuej8mNSPAd75/braqqspKyysduVVVVdWrz/n5+SbJvv76a7/tAwcO9NUPHDt2zM4991ybMGGCffjhh7Zt2zbLysoySbZlyxYzM/vggw9Mkn377bd+x+nXr5+NHDnS3nvvPdu6dat98cUXAdUdBNL2VBs3brTQ0FB77LHH7JNPPrEdO3bY3LlzLTo62tdm8uTJdsMNN/jtd9999/lqRqprEHbv3l3n4/Tv39/OOeecOm8/+clPfG2HDx9eoyZj2bJlFhUVVefxZ82aZYMHD/bbtn//fpNkubm5fturqqrs/PPPtwceeKDO453quuuus7vuuqvO+48dO2aSbN26dXW2efvtt02SnThxwm/7Sy+9ZO3bt7e33367zn2nTJli55133mnHt1p5eblFRETYqlWrzMzshRdesNjYWL96kOo2L730kpnVb+wa8pqYmQ0ePNimT59eY/vhw4d9/wfi4uLs8ccfr9Fm7ty51qVLlxo1PxMnTqxRw/T++++bJPvXv/7lt/2JJ56w6Oho++STT07bT7PGqRkJ6DRNRUWF8vLyNGPGDN+2kJAQJScn13lOMjc3V2lpaX7bUlJSTnspXXl5ucrLy30/FxcXB9JNoM1wuVz1OlXipL59+yosLEx//etf1bNnT0nSt99+qx07dvhmP7788kv985//1IIFC3zn7Ddv3ux3nOrp8+qrHSTpn//8p7Zv3+67HFFSnefQG9vGjRuVkJCgmTNn+rZ9/fXXfm26dOlSYxY4Pz/fV09wwQUXqH379srOztYdd9xR6+MEcpqmttMr69evr1G3caqysrIaf11Xf7/I9+tuNmzYoJ07d9b7L+6qqiq/9/Lvy8/Pl3T6L1jLz89XTEyMX73ESy+9pNtuu02ZmZkaNWpUjX3MTL/61a+0evVq5eTk1Dg1URszk5n5+ls9LqeuLlr9c/W41GfsGvKaHDt2TLt27dLEiRNr3Ne5c2dJ0vvvv69Dhw7phhtuqPE8li9frkmTJtWo+UlKStLMmTNVWVnpu2/9+vXq16+f3ymaxx9/XL/97W+VlZXluxKnyZ0x8pzim2++MUm2ceNGv+1Tp061IUOG1LpPWFiYL2lWy8jIsNjY2DofJz09vUblvJgZAYL2app77rnHEhISLDs72z7//HO74YYbrEOHDr6/GA8dOmRut9umTp1qu3btsjfffNMuvPBCv5mRf/zjH+ZyuWzFihV26NAhKykpMa/Xa+eee67dcsst9tVXX1l2drZdddVVZ5ztKCkpsS1bttiWLVtMki1cuNC2bNlSY/bmdN58801r166dvfTSS7Zz5057+umnrVOnTn4zI+vWrTOXy2UrV660HTt22Jw5cywqKsrvapqHH37YYmJibOXKlbZz507Lzc31uwIlELt377aIiAibOnWqbdu2zTIyMiw0NNRv5uGZZ56xa6+91vdzdna2uVwue+SRR2zHjh2Wl5dnKSkplpCQYGVlZX7Hv+WWWywxMbHWx54+fbpt2LDB9uzZY5999plNnz7dXC6Xvfvuu2ZmtnPnTps7d65t3rzZ9uzZY2+++ab16dPHfvjDH/qO8dZbb9mSJUvs888/t6+++sqee+45i4iIsDlz5vjavPjii9auXTvLyMiwgwcP+m5Hjx71tbn33nstOjracnJy/NpUP59du3bZvHnzbPPmzfb111/bxx9/bKNHj7ZOnTr5ruDatm2beTweu/fee30zbrfccotFR0fbgQMH6j129XlN/t//+3+Wk5Nje/bssY8//tiSk5Otc+fOdujQIV+bZcuWWW5uru3cudP+9Kc/WadOnSwtLa3G6/Dee++ZJNu2bVuN+44ePWpxcXE2ceJE++KLLywzM9MiIiLsv//7v31tFixYYG6321577TW/sSspKan1dTdrnJmRFhlGTpw4YUVFRb5b9bRXY4aRU6e36zvdDDgtWMNISUmJ3XLLLRYREeGbWv7+ZZ6rVq2yXr16mcfjsaSkJHvrrbf8wojZd9PPXbt2NZfL5bu0d/369XbxxRebx+Oxyy+/3HJycs4YRqpP+Xz/Vn1Ms+/+KEpISDjt85o6daqde+651qFDBxs3bpz9/ve/9wsjZmZz5syxuLg4i46OtgcffNB++ctf1ri097HHHrOEhAQLCwuznj172rx5804/oKfxwQcf2MCBA83tdlufPn1s+fLlfvfX9rxeeuklu+KKK+ycc86xLl262A033FDjw+zo0aPWvn17e/7552t93Ntuu80SEhLM7XZbly5d7LrrrvMFETOzffv22Q9/+EPr1KmTeTweO//8823q1Kl+7+vvvPOODRw40Dp06GDnnHOODRgwwBYvXux3qmTEiBFnfO1qu1+Sbyy++eYbu/766y02NtbCwsLsvPPOswkTJtiXX37p95zeffdd32XjMTExdu2119Y4dVWfsTvTazJu3Djr1q2bud1u69Gjh40bN67G5enTpk2zuLg4CwsLswsuuMCeeuqpWj+7xo8fb0OHDq31NTL77hLmq6++2jwej/Xo0cMWLFjgd39CQkKtY5eenl7nMRsjjLjM6l9lU1FRoYiICL322mt+xUupqak6evSo3nzzzRr79OzZU2lpaX7LPqenp+uNN96osZJdXYqLixUdHa2ioiJFRUXVt7tAq3PixAnt2bNHvXv3bvqCsjYuNTVVLperXiuYAm3Z6d6X6vv5HdDVNG63W4MGDVJ2drZvW1VVlbKzs+s8/5WUlOTXXjrz+TIAcJKZKScnx3dJJ4CmFXDlW1pamlJTUzV48GANGTJEixYtUmlpqW9hlUmTJqlHjx6aP3++JOn+++/XiBEj9NRTT2nUqFHKzMzU5s2b/ZafBYCWxOVy1ShGBdB0Ag4j48aN0+HDhzVnzhwVFBRo4MCBWrdunW/hlH379vlVFw8dOlSrVq3SrFmz9Jvf/EYXXHCB3njjjSZfjAgAAASHgGpGnELNCPAdakYAtDTNXjMCAADQ2AgjQBBqji+BA4D6aIz3o5a9dCMAP263WyEhITpw4IC6dOkit9vtt0IkADQXM1NFRYUOHz6skJAQv28DDhRhBAgiISEh6t27tw4ePKgDBw443R0AUEREhHr27Fnrl/bVF2EECDJut1s9e/bUyZMn/b6nBQCaW2hoqNq1a3fWM7SEESAIuVwuhYWF1fgiLAAIRhSwAgAARxFGAACAowgjAADAUUFRM1K9SGxxcbHDPQEAAPVV/bl9psXegyKMlJSUSJLi4+Md7gkAAAhUSUmJoqOj67w/KL6bpqqqSgcOHFBkZGSjLvBUXFys+Ph47d+/n++8aUKMc/NhrJsH49w8GOfm0ZTjbGYqKSlR9+7dT7sOSVDMjISEhOi8885rsuNHRUXxi94MGOfmw1g3D8a5eTDOzaOpxvl0MyLVKGAFAACOIowAAABHtekw4vF4lJ6eLo/H43RXWjXGufkw1s2DcW4ejHPzaAnjHBQFrAAAoPVq0zMjAADAeYQRAADgKMIIAABwFGEEAAA4qtWHkYyMDPXq1Uvh4eFKTEzUpk2bTtv+1Vdf1UUXXaTw8HBddtllWrt2bTP1NLgFMs5LlizR8OHDFRMTo5iYGCUnJ5/xdcG/Bfo7XS0zM1Mul0tjxoxp2g62EoGO89GjRzVlyhR169ZNHo9HF154Ie8f9RDoOC9atEj9+vVT+/btFR8frwcffFAnTpxopt4Gpw8//FCjR49W9+7d5XK59MYbb5xxn5ycHF155ZXyeDw6//zztWLFiqbtpLVimZmZ5na7bdmyZfb3v//d7rzzTuvYsaMVFhbW2v7jjz+20NBQe/zxx23r1q02a9YsCwsLs88//7yZex5cAh3nCRMmWEZGhm3ZssW2bdtmt956q0VHR9s//vGPZu558Al0rKvt2bPHevToYcOHD7cbb7yxeTobxAId5/Lychs8eLD99Kc/tY8++sj27NljOTk5lp+f38w9Dy6BjvOLL75oHo/HXnzxRduzZ49lZWVZt27d7MEHH2zmngeXtWvX2syZM+311183SbZ69erTtt+9e7dFRERYWlqabd261Z555hkLDQ21devWNVkfW3UYGTJkiE2ZMsX3s9frte7du9v8+fNrbT927FgbNWqU37bExES7++67m7SfwS7Qcf6+kydPWmRkpK1cubKputhqNGSsT548aUOHDrUXXnjBUlNTCSP1EOg4/+EPf7A+ffpYRUVFc3WxVQh0nKdMmWLXXnut37a0tDQbNmxYk/azNalPGHnooYfskksu8ds2btw4S0lJabJ+tdrTNBUVFcrLy1NycrJvW0hIiJKTk5Wbm1vrPrm5uX7tJSklJaXO9mjYOH9fWVmZKisr1alTp6bqZqvQ0LGeO3euYmNjdfvttzdHN4NeQ8b5rbfeUlJSkqZMmaK4uDhdeumlmjdvnrxeb3N1O+g0ZJyHDh2qvLw836mc3bt3a+3atfrpT3/aLH1uK5z4LAyKL8priCNHjsjr9SouLs5ve1xcnL788sta9ykoKKi1fUFBQZP1M9g1ZJy/b9q0aerevXuNX374a8hYf/TRR1q6dKny8/OboYetQ0PGeffu3Xr//fd18803a+3atdq5c6fuu+8+VVZWKj09vTm6HXQaMs4TJkzQkSNHdPXVV8vMdPLkSd1zzz36zW9+0xxdbjPq+iwsLi7W8ePH1b59+0Z/zFY7M4LgsGDBAmVmZmr16tUKDw93ujutSklJiSZOnKglS5aoc+fOTnenVauqqlJsbKyef/55DRo0SOPGjdPMmTO1ePFip7vWquTk5GjevHl67rnn9Omnn+r111/XmjVr9OijjzrdNZylVjsz0rlzZ4WGhqqwsNBve2Fhobp27VrrPl27dg2oPRo2ztWefPJJLViwQO+9954uv/zypuxmqxDoWO/atUt79+7V6NGjfduqqqokSe3atdP27dvVt2/fpu10EGrI73S3bt0UFham0NBQ37aLL75YBQUFqqiokNvtbtI+B6OGjPPs2bM1ceJE3XHHHZKkyy67TKWlpbrrrrs0c+ZMhYTw93VjqOuzMCoqqklmRaRWPDPidrs1aNAgZWdn+7ZVVVUpOztbSUlJte6TlJTk116S1q9fX2d7NGycJenxxx/Xo48+qnXr1mnw4MHN0dWgF+hYX3TRRfr888+Vn5/vu91www265pprlJ+fr/j4+ObsftBoyO/0sGHDtHPnTl/Yk6QdO3aoW7duBJE6NGScy8rKagSO6gBofM1ao3Hks7DJSmNbgMzMTPN4PLZixQrbunWr3XXXXdaxY0crKCgwM7OJEyfa9OnTfe0//vhja9eunT355JO2bds2S09P59Leegh0nBcsWGBut9tee+01O3jwoO9WUlLi1FMIGoGO9fdxNU39BDrO+/bts8jISPvlL39p27dvt7fffttiY2Ptsccec+opBIVAxzk9Pd0iIyPtpZdest27d9u7775rffv2tbFjxzr1FIJCSUmJbdmyxbZs2WKSbOHChbZlyxb7+uuvzcxs+vTpNnHiRF/76kt7p06datu2bbOMjAwu7T1bzzzzjPXs2dPcbrcNGTLE/vKXv/juGzFihKWmpvq1f+WVV+zCCy80t9ttl1xyia1Zs6aZexycAhnnhIQEk1Tjlp6e3vwdD0KB/k6fijBSf4GO88aNGy0xMdE8Ho/16dPHfvvb39rJkyebudfBJ5BxrqystIcfftj69u1r4eHhFh8fb/fdd599++23zd/xIPLBBx/U+p5bPbapqak2YsSIGvsMHDjQ3G639enTx5YvX96kfXSZMbcFAACc02prRgAAQHAgjAAAAEcRRgAAgKMIIwAAwFGEEQAA4CjCCAAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUf8fCoqoc6XSeIMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auc score of 0.874 out of 1 indicate a good performance. If an auc score is 0.5, meaning it is no different from random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Partitioning the data \n",
    "There are limitation of only using 1 single training set, here are 3 better ways to split our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-sampling\n",
    "Random sub-sampling repeatedly partitions the data into random training and test sets in a specified ratio.\n",
    "\n",
    "![picture](2.png){width=550 height=300}\n",
    "\n",
    "The figure shows us that we train the model with each training set and estimate the accuracy using the corresponding test set. Finally, we average the accuracies to get an averaged estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Sampling\n",
    "A splitting technique that ensures each class (or stratum) in the dataset is proportionate in both training and testing sets. In other words, the distribution of classes in training and testing sets mirrors the distribution of the original dataset. This technique is useful in imbalanced dataset. \n",
    "\n",
    "![picture](3.png){width=550 height=300}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "Cross-validation is one of the most popular splitting technique. The core idea is to partition the data into $k$ equal sized sub-samples. It is an iterative process where we leave one sub-sample out for the test set, and train on the rest of the sub-samples. \n",
    "\n",
    "![picture](4.png){width=550 height=300}\n",
    "\n",
    "The picture shows us that we divide our data into 5 partitions. The first iteration trains on $s_2, s_3, s_4, s_5$ and test on $s_1$. The second iteration trains on $s_1, s_3, s_4, s_5$ and test on $s_2$. The same goes for 3 other iterations. After 5 iterations is completed, we take the average accuracy of the 5 times we test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Finding the best hyperparameters\n",
    "![picture](5.png){width=550 height=400}\n",
    "\n",
    "The validation set is used to evaluate the given model and help fine-tune the model hyperparameters. The validation part is part of the training process, there are $3$ methods that automatically split our training data further into training data to train the given model, and validating data to validate and help choose the best hyperparmeters. \n",
    "\n",
    "Note that the following methods can include internal cross-validation. Instead of train/test cross-validate, we cross-validate the train/validate set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exhaustive Search Methods\n",
    "- **Grid Search**: the possible values of hyperparameters are defined in a few sets. These sets are then combined bia Cartesian product to form a multidimensional grid. Then we will try all the parameters in the grid to find the best ones. \n",
    "- **Random Search**: a variant of grid search where we try random points instead of all.\n",
    "\n",
    "Although this algorithms run parallel and easy to implement. There are a few drawback:\n",
    "- It can take lots of time and computational power if the hyperparameter search space is large.\n",
    "- There is not guarantee of finding the local maxima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization\n",
    "Banerjee, P. (n.d.) on Kaggle provides the information for Bayesian Optimization.\n",
    "\n",
    "Bayesian Optimization is a probabilistic model that yields better performance and requires less time than Grid Search. The model focuses of past evaluations when choosing the optimal set of hyperparameters, for that it is less time consuming. \n",
    "\n",
    "##### Bayesian Optimization Method\n",
    "Bayesian Optimization is called Sequential **Model-Based Optimization (SMBO)**, where it builds a surrogate function that evaluates the input values based on expected improvement while focuses values that perform well in the past, and excludes poorer ones. \n",
    "\n",
    "There are several libraries support Bayesian optimization, like Spearmint (uses Gaussian Process surrogate), SMAC (uses Random Forest Regression), Hyperopt (uses Tree Parzen Estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Imbalance dataset\n",
    "For classification dataset, there one label might have too many data while the other not so much. The machine learning models work best when the labels are roughly the same.\n",
    "\n",
    "The issues with imbalanced dataset can be:\n",
    "- Since the test data contains a few sample of the minority class, even a dumb classifier can get high accuracy. But this problem can be solved with other evaluation metrics.\n",
    "- When random subsampling, the class proportion might not be maintained, but this can be solved via Stratified Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "We have 2 approaches, either to modify the data or improve the algorithm. \n",
    "##### Data level\n",
    "- **Oversampling** the data from minority class\n",
    "- **Undersampling** the data from majority class\n",
    "##### Algorithmic level\n",
    "- adjusting the costs\n",
    "- adjusting the decision threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6: Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Kernel trick and non-linear SVM \n",
    "![picture](6.png){width=700 height=250}\n",
    "\n",
    "Sometimes, by transforming data points that are not linearly separate into a higher dimension, those data might be linearly separable. \n",
    "\n",
    "The information below is from Wilimitis, D. (2019).\n",
    "#### Intro to Support Vector Classification (SVC)\n",
    "SVC goal is to maximize the margin (the distance separating the closest pair of data points belonging to different classes). Those data points are called support vectors because they \"support\" the decision boundary. The decision boundary is the *optimal separating hyperplane*.\n",
    "\n",
    "![picture](7.png){width=500 height=500}\n",
    "\n",
    "#### The Kernel Trick\n",
    "To apply to SVC, our data needs to be linear to be separated, that is where the Kernel Trick comes in, where it projects the original non-linear data into higher dimension space where data may become linearly separable. This concept will be cover more in the next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model complexity\n",
    "Since I have already gone over the linear regression and logistic regression in week 5, I won't go over it again (the mathematical concept) in week 6.\n",
    "\n",
    "Model complexity is the measure of how complex our model is.\n",
    "Under-fitting means that our model is very simple. Over-fitting means our model is too complex and learn the noise of our data.\n",
    "\n",
    "![picture](9.png){width=700 height=500}\n",
    "\n",
    "This picture provides a good diagnose on the complexity of our model. The main thing we need to focus on is the train error and cross validation error. Our goal is to find the sweet spot where Risk = bias + variance + noise is the minimum.\n",
    "\n",
    "Here are a few steps to try to find that sweet spot.\n",
    "\n",
    "So we know that bias means underfit, variance means overfit, noise is the Irreducible error inherent in the data. The first thing we need to do is to choose a model family, like decision tree or neural network. Then we use k-fold cross-validation and calculate the model's error, which includes bias and variance components. Then we plot the bias-variance validation.\n",
    "- **Training error**: typically decrease with model's complexity (lower bias, higher variance).\n",
    "- **Validation error**: decreases initially but increase as the model becomes overfit (higher variance).\n",
    "\n",
    "The sweet spot is where the validation error is minimized.\n",
    "\n",
    "If our model is too complex, we can use **Regularisation** to control the model complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Regularised linear models\n",
    "Regulariser is addition term of the loss function to avoid overfitting. It does not allow regression coefficients (or weights) to take excessively large value. The formula will now become:\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, \\mathbf{x}^T_i\\mathbf{w}) + \\lambda \\text{Regulariser}(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "There are 2 things we want our model to discourage. One is **high weights** because one small change in those features can lead to large changes in the prediction. Second is **irrelevant features**, so we want to discard those features.\n",
    "\n",
    "There are 2 options.\n",
    "- **Option 1** ($l_1$-norm): this option encourages 0 weights, this means discarding irrelevant data. \n",
    "$$\n",
    "\\text{Regulariser}(\\mathbf{w}) = \\sum_j |w_j| = ||\\mathbf{w}||_1\n",
    "$$\n",
    "\n",
    "- **Option 2** ($l_2$-norm): penalises large weights\n",
    "$$\n",
    "\\text{Regulariser}(\\mathbf{w}) = \\sum_j |w_j|^2 = ||\\mathbf{w}||_2\n",
    "$$\n",
    "\n",
    "These 2 options are part of L1 and L2 methods.\n",
    "\n",
    "1. **L1 Regularisation (LASSO)**\n",
    "\n",
    "This is a regression analysis that shrinks some coefficient to zero to discard them.\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, \\mathbf{x}^T_i\\mathbf{w}) + \\lambda_1 ||\\mathbf{w}||_1\n",
    "$$\n",
    "\n",
    "2. **L2 Regularisation (Ridge)**\n",
    "\n",
    "This is a regression analysis that prevents overfitting by shrinking coefficients but don't remove them.\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, \\mathbf{x}^T_i\\mathbf{w}) + \\lambda_2 ||\\mathbf{w}||_2\n",
    "$$\n",
    "\n",
    "Ridge is a special case of **Elastic Net** where $\\lambda_1 = \\lambda_2 =0$.\n",
    "\n",
    "Elastic Net combines both LASSO and Ridge where it overcomes the LASSO's limit of selecting at most $n$ variables before it saturates. The formula for Elastic Net is:\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, \\mathbf{x}^T_i\\mathbf{w}) + \\lambda_1 ||\\mathbf{w}||_1 + \\lambda_2 ||\\mathbf{w}||_2\n",
    "$$\n",
    "\n",
    "The effect of Regularisation increases biases and reduces variance helps us choose the right trade-off, or the sweet spot where the validation error is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Feature selection with Regularised linear models\n",
    "Lasso and Ridge are regularization techniques for learn models, and not applicable for non-linear model, though the concept of regularization can be extended to non-linear models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "<img src=\"quiz1.png\" width=\"700\" height=\"500\" alt=\"picture\">\n",
    "\n",
    "<img src=\"quiz2.png\" width=\"700\" height=\"500\" alt=\"picture\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "Week 5, 6 have taught me the basic of supervised learning that is linear regression and logistic regression. It also shows me techniques to split data, metrics to test model, and how to diagnose the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "GeeksforGeeks (2024). Dataset for Linear Regression. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/dataset-for-linear-regression/ [Accessed 16 Aug. 2024].\n",
    "\n",
    "Ng, A. (n.d.). Machine Learning Specialization. [online] DeepLearning.AI. Available at: https://www.deeplearning.ai/courses/machine-learning-specialization/.\n",
    "\n",
    "Kaggle (n.d.). Pima Indians Diabetes Database. [online] www.kaggle.com. Available at: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database.\n",
    "\n",
    "Navlani, A. (2019). Python Logistic Regression Tutorial with Sklearn & Scikit. [online] www.datacamp.com. Available at: https://www.datacamp.com/tutorial/understanding-logistic-regression-python.\n",
    "\n",
    "Banerjee, P. (n.d.). Bayesian Optimization using Hyperopt. [online] kaggle.com. Available at: https://www.kaggle.com/code/prashant111/bayesian-optimization-using-hyperopt/notebook.\n",
    "\n",
    "Wilimitis, D. (2019). The Kernel Trick. [online] Medium. Available at: https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f.\n",
    "\n",
    "Bhavesh Bhatt (2020). Kernel Trick in Support Vector Machine (SVM). [online] YouTube. Available at: https://www.youtube.com/watch?v=aglNRO18R7g [Accessed 21 Aug. 2024]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
